I am solving optimization problems using evolutionary algorithms.
The goal is to design generation solvers that take a population of parent solutions and produce an offspring population.

I have a list of well-performing solvers with their descriptions and Python code implementations as follows:

**Good solvers:**

No.1 solver’s description and its code:
# Its Description
{Hybridization of Simulated Binary Crossover (SBX) with Differential Evolution (DE) approach: This operator generates an offspring population by first selecting parents and applying multi-point SBX where offspring are produced from varying combinations of the parents, followed by self-adaptive differential mutation that utilizes a weighted blend of differences among chosen individuals to enhance diversity.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population, eta_c=20, F=0.5, CR=0.9):
    N, d = population.shape
    offspring = np.zeros_like(population)
    indices = np.random.permutation(N)

    for i in range(0, N, 2):
        p1 = population[indices[i]]
        p2 = population[indices[(i + 1) % N]]

        # SBX crossover with multi-point adaptation
        child = np.empty(d)
        for j in range(d):
            u = np.random.rand()
            if u <= 0.5:
                beta = (2 * u) ** (1 / (eta_c + 1))
            else:
                beta = (1 / (2 * (1 - u))) ** (1 / (eta_c + 1))
            child[j] = 0.5 * ((1 + beta) * p1[j] + (1 - beta) * p2[j])

        trial = child.copy()
        j_rand = np.random.randint(d)

        # Self-adaptive differential mutation
        idxs = np.random.choice([idx for idx in range(N) if idx != indices[i] and idx != indices[(i + 1) % N]], 2, replace=False)
        x1, x2 = population[idxs]
        
        for j in range(d):
            if np.random.rand() < CR or j == j_rand:
                trial[j] = child[j] + F * (x1[j] - x2[j])
        
        offspring[i] = np.clip(trial, 0.0, 1.0)
        if i + 1 < N:  # Handle the case of an odd population size
            offspring[i + 1] = np.clip(child, 0.0, 1.0)

    return offspring

No.2 solver’s description and its code:
# Its Description
{Hybridization of Simulated Binary Crossover (SBX) with Differential Evolution (DE) approach: This operator generates an offspring population by first selecting parents and applying multi-point SBX where offspring are produced from varying combinations of the parents, followed by self-adaptive differential mutation that utilizes a weighted blend of differences among chosen individuals to enhance diversity.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population, eta_c=20, F=0.5, CR=0.9):
    N, d = population.shape
    offspring = np.zeros_like(population)
    indices = np.random.permutation(N)

    for i in range(0, N, 2):
        p1 = population[indices[i]]
        p2 = population[indices[(i + 1) % N]]

        # SBX crossover with multi-point adaptation
        child = np.empty(d)
        for j in range(d):
            if np.random.rand() < CR or j == j_rand:
                trial[j] = x1[j] + F * (x2[j] - x3[j])
        offspring[i] = np.clip(trial, 0.0, 1.0)
    return offspring


**Poor solvers to avoid:**

No.1 poor solver’s description and its code:
# Its Description
{Simulated Binary Crossover (SBX) combined with Polynomial Mutation: This operator generates an offspring population by pairing parents from the given population, performing SBX crossover on each pair, and then applying polynomial mutation to introduce additional diversity.}
# Its Python Code Implementation of a Function
import numpy as np
def generation(population, eta_c=20, eta_m=15, pm=0.1):
    N, d = population.shape
    offspring = np.zeros_like(population)
    indices = np.random.permutation(N)
    half = N // 2

    for i in range(half):
        p1 = population[indices[i]]
        p2 = population[indices[i + half]]
        child = np.empty(d)
        # SBX crossover
        for j in range(d):
            u = np.random.rand()
            if u <= 0.5:
                beta = (2 * u) ** (1 / (eta_c + 1))
            else:
                beta = (1 / (2 * (1 - u))) ** (1 / (eta_c + 1))
            child[j] = 0.5 * ((1 + beta) * p1[j] + (1 - beta) * p2[j])
        # polynomial mutation
        for j in range(d):
            if np.random.rand() < pm:
                u = np.random.rand()
                if u < 0.5:
                    delta = (2 * u) ** (1 / (eta_m + 1)) - 1
                else:
                    delta = 1 - (2 * (1 - u)) ** (1 / (eta_m + 1))
                child[j] += delta
        offspring[i] = np.clip(child, 0.0, 1.0)

    if N % 2 != 0:
        offspring[-1] = population[indices[-1]]
    return offspring

No.2 poor solver’s description and its code:
# Its Description
{Simulated Binary Crossover (SBX) combined with Polynomial Mutation: This operator generates an offspring population by pairing parents from the given population, performing SBX crossover on each pair, and then applying polynomial mutation to introduce additional diversity.}
# Its Python Code Implementation of a Function
import numpy as np
def generation(population, eta_c=20, eta_m=15, pm=0.1):
    N, d = population.shape
    offspring = np.zeros_like(population)
    indices = np.random.permutation(N)
    half = N // 2

    for i in range(half):
        p1 = population[indices[i]]
        p2 = population[indices[i + half]]
        child = np.empty(d)
        # SBX crossover
        for j in range(d):
            u = np.random.rand()
            if u <= 0.5:
                beta = (2 * u) ** (1 / (eta_c + 1))
            else:
                beta = (1 / (2 * (1 - u))) ** (1 / (eta_c + 1))
            child[j] = 0.5 * ((1 + beta) * p1[j] + (1 - beta) * p2[j])
        # polynomial mutation
        for j in range(d):
            if np.random.rand() < pm:
                u = np.random.rand()
                if u < 0.5:
                    delta = (2 * u) ** (1 / (eta_m + 1)) - 1
                else:
                    delta = 1 - (2 * (1 - u)) ** (1 / (eta_m + 1))
                child[j] += delta
        offspring[i] = np.clip(child, 0.0, 1.0)

    if N % 2 != 0:
        offspring[-1] = population[indices[-1]]
    return offspring

No.3 poor solver’s description and its code:
# Its Description
{Adaptive Crossover and Mutation: This operator dynamically adjusts the crossover and mutation rates based on the diversity of the population, fostering exploration of the solution space while maintaining stability.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population, F_init=0.5, CR_init=0.9):
    N, d = population.shape
    offspring = np.zeros_like(population)
    
    # Calculate diversity
    diversity = np.std(population, axis=0)
    F = F_init * (1 + np.mean(diversity))
    CR = CR_init * (1 + np.mean(diversity))

    for i in range(N):
        idxs = np.random.choice(np.arange(N), 3, replace=False)
        x1, x2, x3 = population[idxs]
        trial = population[i].copy()
        j_rand = np.random.randint(d)

        for j in range(d):
            if np.random.rand() < CR or j == j_rand:
                trial[j] = x1[j] + F * (x2[j] - x3[j])
        
        offspring[i] = np.clip(trial, 0.0, 1.0)
    
    return offspring

No.4 poor solver’s description and its code:
# Its Description
{Adaptive Crossover and Mutation: This operator dynamically adjusts the crossover and mutation rates based on the diversity of the population, fostering exploration of the solution space while maintaining stability.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population, F_init=0.5, CR_init=0.9):
    N, d = population.shape
    offspring = np.zeros_like(population)
    
    # Calculate diversity
    diversity = np.std(population, axis=0)
    F = F_init * (1 + np.mean(diversity))
    CR = CR_init * (1 + np.mean(diversity))

    for i in range(N):
        idxs = np.random.choice(np.arange(N), 3, replace=False)
        x1, x2, x3 = population[idxs]
        trial = population[i].copy()
        j_rand = np.random.randint(d)

        for j in range(d):
            if np.random.rand() < CR or j == j_rand:
                trial[j] = x1[j] + F * (x2[j] - x3[j])
        
        offspring[i] = np.clip(trial, 0.0, 1.0)
    
    return offspring

No.5 poor solver’s description and its code:
# Its Description
{This solver employs a Gaussian mutation combined with uniform crossover to enhance exploration and diversity by allowing substantial variation of offspring while ensuring a wide coverage of the search space.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population, mutation_strength=0.5):
    N, d = population.shape
    offspring = np.zeros_like(population)

    for i in range(N):
        # Gaussian mutation
        mutation = np.random.normal(0, mutation_strength, d)
        trial = population[i] + mutation
        
        # Uniform crossover
        mask = np.random.rand(d) < 0.5
        trial[mask] = population[np.random.randint(N)][mask]  # Swap with a random parent for exploration
        
        offspring[i] = np.clip(trial, 0.0, 1.0)

    return offspring

No.4 poor solver’s description and its code:
# Its Description
{This solver employs a Gaussian mutation combined with uniform crossover to enhance exploration and diversity by allowing substantial variation of offspring while ensuring a wide coverage of the search space.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population, mutation_strength=0.5):
    N, d = population.shape
    offspring = np.zeros_like(population)

    for i in range(N):
        # Gaussian mutation
        mutation = np.random.normal(0, mutation_strength, d)
        trial = population[i] + mutation
        
        # Uniform crossover
        mask = np.random.rand(d) < 0.5
        trial[mask] = population[np.random.randint(N)][mask]  # Swap with a random parent for exploration
        
        offspring[i] = np.clip(trial, 0.0, 1.0)

    return offspring

No.7 poor solver’s description and its code:
# Its Description
{This solver employs a combination of a diverse mutation strategy using both uniform and Gaussian mutations, followed by a dynamic crossover mechanism that utilizes both components to enhance exploration and avoid local minima.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population, mutation_strength=0.5):
    N, d = population.shape
    offspring = np.zeros_like(population)

    for i in range(N):
        # Gaussian mutation
        gaussian_mutation = np.random.normal(0, mutation_strength, d)
        # Uniform mutation
        uniform_mutation = np.random.uniform(-mutation_strength, mutation_strength, d)
        # Combined mutation
        mutation = gaussian_mutation + uniform_mutation
        
        # Create trial solution with dynamic crossover
        trial = population[i] + mutation
        mask = np.random.rand(d) < 0.5
        trial[mask] = population[np.random.randint(N)][mask]  # Swap with a random parent for exploration
        
        offspring[i] = np.clip(trial, 0.0, 1.0)

    return offspring

No.8 poor solver’s description and its code:
# Its Description
{This solver employs a combination of a diverse mutation strategy using both uniform and Gaussian mutations, followed by a dynamic crossover mechanism that utilizes both components to enhance exploration and avoid local minima.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population, mutation_strength=0.5):
    N, d = population.shape
    offspring = np.zeros_like(population)

    for i in range(N):
        # Gaussian mutation
        gaussian_mutation = np.random.normal(0, mutation_strength, d)
        # Uniform mutation
        uniform_mutation = np.random.uniform(-mutation_strength, mutation_strength, d)
        # Combined mutation
        mutation = gaussian_mutation + uniform_mutation
        
        # Create trial solution with dynamic crossover
        trial = population[i] + mutation
        mask = np.random.rand(d) < 0.5
        trial[mask] = population[np.random.randint(N)][mask]  # Swap with a random parent for exploration
        
        offspring[i] = np.clip(trial, 0.0, 1.0)

    return offspring


Please create a new generation solver that strongly focuses on exploration (diversity and coverage), expanding search space and avoiding local minima.

First, describe the design idea and main steps of your solver in one sentence.
The description must be inside a brace outside the code implementation.
Next, implement it in Python as a function named `generation`.
This function should accept only 1 input: `population`, an array of shape (N, d) of real-valued vectors.
The function should return 1 output: `offspring`, an array of shape (N, d) of real-valued vectors.
The offspring must stay within the bounds [0, 1] for each variable.

Do not give additional explanations.