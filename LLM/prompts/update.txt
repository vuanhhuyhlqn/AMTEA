I am solving optimization problems using evolutionary algorithms.
The goal is to design generation solvers that take a population of parent solutions and produce an offspring population.

I have a list of well-performing solvers with their descriptions and Python code implementations as follows:

**Good solvers:**

No.1 solver’s description and its code:
# Its Description
{Hybrid Crossover Using Adaptive Blend and Self-Adaptive Mutation: This operator combines Gaussian blend crossover with a self-adaptive mutation strategy based on the variance of parents' traits, enhancing exploration and exploitation features in the offspring generation.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population):
    N, d = population.shape
    offspring = np.zeros_like(population)
    
    for i in range(N):
        p1, p2 = population[np.random.choice(N, 2, replace=False)]
        # Gaussian blend crossover
        alpha = np.random.uniform(0, 1, d)
        child = alpha * p1 + (1 - alpha) * p2
        
        # Self-adaptive mutation based on variance
        mutation_strength = np.std([p1, p2], axis=0) * np.random.uniform(0, 1, d)
        mutation_direction = np.random.normal(0, 1, d)  # Gaussian noise
        mutation = mutation_strength * mutation_direction
        child += mutation
        
        # Clip to ensure within [0, 1]
        offspring[i] = np.clip(child, 0.0, 1.0)
    
    return offspring

No.2 solver’s description and its code:
# Its Description
{Hybrid Crossover Using Adaptive Blend and Self-Adaptive Mutation: This operator combines Gaussian blend crossover with a self-adaptive mutation strategy based on the variance of parents' traits, enhancing exploration and exploitation features in the offspring generation.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population):
    N, d = population.shape
    offspring = np.zeros_like(population)
    
    for i in range(N):
        p1, p2 = population[np.random.choice(N, 2, replace=False)]
        # Gaussian blend crossover
        alpha = np.random.uniform(0, 1, d)
        child = alpha * p1 + (1 - alpha) * p2
        
        # Self-adaptive mutation based on variance
        mutation_strength = np.std([p1, p2], axis=0) * np.random.uniform(0, 1, d)
        mutation_direction = np.random.normal(0, 1, d)  # Gaussian noise
        mutation = mutation_strength * mutation_direction
        child += mutation
        
        # Clip to ensure within [0, 1]
        offspring[i] = np.clip(child, 0.0, 1.0)
    
    return offspring

No.3 solver’s description and its code:
# Its Description
{Adaptive Crossover with Variable Blend and Controlled Gaussian Mutation: This operator employs a variable blend crossover mechanism influenced by the traits of parents and applies a controlled Gaussian mutation strategy that retains offspring within specified bounds while encouraging diversity.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population):
    N, d = population.shape
    offspring = np.zeros_like(population)
    
    for i in range(N):
        p1, p2 = population[np.random.choice(N, 2, replace=False)]
        # Adaptive blend crossover
        alpha = np.random.uniform(0, 1, d) * np.random.uniform(0.5, 1.5, d)
        child = alpha * p1 + (1 - alpha) * p2
        
        # Controlled Gaussian mutation
        mutation_size = np.clip(np.std([p1, p2], axis=0) * np.random.uniform(0.1, 0.3, d), 0, 0.1)
        mutation_direction = np.random.normal(0, 1, d)
        mutation = mutation_size * mutation_direction
        child += mutation
        
        # Clip to ensure within [0, 1]
        offspring[i] = np.clip(child, 0.0, 1.0)
    
    return offspring

No.4 solver’s description and its code:
# Its Description
{Adaptive Crossover with Variable Blend and Controlled Gaussian Mutation: This operator employs a variable blend crossover mechanism influenced by the traits of parents and applies a controlled Gaussian mutation strategy that retains offspring within specified bounds while encouraging diversity.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population):
    N, d = population.shape
    offspring = np.zeros_like(population)
    
    for i in range(N):
        p1, p2 = population[np.random.choice(N, 2, replace=False)]
        # Adaptive blend crossover
        alpha = np.random.uniform(0, 1, d) * np.random.uniform(0.5, 1.5, d)
        child = alpha * p1 + (1 - alpha) * p2
        
        # Controlled Gaussian mutation
        mutation_size = np.clip(np.std([p1, p2], axis=0) * np.random.uniform(0.1, 0.3, d), 0, 0.1)
        mutation_direction = np.random.normal(0, 1, d)
        mutation = mutation_size * mutation_direction
        child += mutation
        
        # Clip to ensure within [0, 1]
        offspring[i] = np.clip(child, 0.0, 1.0)
    
    return offspring

No.5 solver’s description and its code:
# Its Description
{Adaptive Crossover with Dynamic Blend and Gaussian Noise Mutation: This operator utilizes a dynamic blend factor influenced by parent traits and applies Gaussian noise mutation with a varying intensity based on the average distance between parents, promoting flexibility and diversity in offspring generation.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population):
    N, d = population.shape
    offspring = np.zeros_like(population)
    
    for i in range(N):
        p1, p2 = population[np.random.choice(N, 2, replace=False)]
        # Dynamic blend factor
        average_distance = np.abs(p1 - p2)
        blend_factor = np.clip(np.random.uniform(0.5, 1.5, d) * average_distance, 0, 1)
        child = blend_factor * p1 + (1 - blend_factor) * p2
        
        # Gaussian noise mutation
        mutation_strength = np.clip(np.mean(average_distance) * np.random.uniform(0.05, 0.2), 0, 0.1)
        mutation = mutation_strength * np.random.normal(0, 1, d)
        child += mutation
        
        # Clip to ensure within [0, 1]
        offspring[i] = np.clip(child, 0.0, 1.0)
    
    return offspring

No.6 solver’s description and its code:
# Its Description
{Adaptive Crossover with Dynamic Blend and Gaussian Noise Mutation: This operator utilizes a dynamic blend factor influenced by parent traits and applies Gaussian noise mutation with a varying intensity based on the average distance between parents, promoting flexibility and diversity in offspring generation.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population):
    N, d = population.shape
    offspring = np.zeros_like(population)
    
    for i in range(N):
        p1, p2 = population[np.random.choice(N, 2, replace=False)]
        # Dynamic blend factor
        average_distance = np.abs(p1 - p2)
        blend_factor = np.clip(np.random.uniform(0.5, 1.5, d) * average_distance, 0, 1)
        child = blend_factor * p1 + (1 - blend_factor) * p2
        
        # Gaussian noise mutation
        mutation_strength = np.clip(np.mean(average_distance) * np.random.uniform(0.05, 0.2), 0, 0.1)
        mutation = mutation_strength * np.random.normal(0, 1, d)
        child += mutation
        
        # Clip to ensure within [0, 1]
        offspring[i] = np.clip(child, 0.0, 1.0)
    
    return offspring


**Poor solvers to avoid:**

No.1 poor solver’s description and its code:
# Its Description
{Simulated Binary Crossover (SBX) combined with Polynomial Mutation: This operator generates an offspring population by pairing parents from the given population, performing SBX crossover on each pair, and then applying polynomial mutation to introduce additional diversity.}
# Its Python Code Implementation of a Function
import numpy as np
def generation(population, eta_c=2, eta_m=5, pm=0.1):
    N, d = population.shape
    offspring = np.zeros_like(population)
    indices = np.random.permutation(N)
    half = N // 2

    for i in range(half):
        p1 = population[indices[i]]
        p2 = population[indices[i + half]]
        child = np.empty(d)
        # SBX crossover
        for j in range(d):
            u = np.random.rand()
            if u <= 0.5:
                beta = (2 * u) ** (1 / (eta_c + 1))
            else:
                beta = (1 / (2 * (1 - u))) ** (1 / (eta_c + 1))
            child[j] = 0.5 * ((1 + beta) * p1[j] + (1 - beta) * p2[j])
        # polynomial mutation
        for j in range(d):
            if np.random.rand() < pm:
                u = np.random.rand()
                if u < 0.5:
                    delta = (2 * u) ** (1 / (eta_m + 1)) - 1
                else:
                    delta = 1 - (2 * (1 - u)) ** (1 / (eta_m + 1))
                child[j] += delta
        offspring[i] = np.clip(child, 0.0, 1.0)

    if N % 2 != 0:
        offspring[-1] = population[indices[-1]]
    return offspring

No.2 poor solver’s description and its code:
# Its Description
{Simulated Binary Crossover (SBX) combined with Polynomial Mutation: This operator generates an offspring population by pairing parents from the given population, performing SBX crossover on each pair, and then applying polynomial mutation to introduce additional diversity.}
# Its Python Code Implementation of a Function
import numpy as np
def generation(population, eta_c=2, eta_m=5, pm=0.1):
    N, d = population.shape
    offspring = np.zeros_like(population)
    indices = np.random.permutation(N)
    half = N // 2

    for i in range(half):
        p1 = population[indices[i]]
        p2 = population[indices[i + half]]
        child = np.empty(d)
        # SBX crossover
        for j in range(d):
            u = np.random.rand()
            if u <= 0.5:
                beta = (2 * u) ** (1 / (eta_c + 1))
            else:
                beta = (1 / (2 * (1 - u))) ** (1 / (eta_c + 1))
            child[j] = 0.5 * ((1 + beta) * p1[j] + (1 - beta) * p2[j])
        # polynomial mutation
        for j in range(d):
            if np.random.rand() < pm:
                u = np.random.rand()
                if u < 0.5:
                    delta = (2 * u) ** (1 / (eta_m + 1)) - 1
                else:
                    delta = 1 - (2 * (1 - u)) ** (1 / (eta_m + 1))
                child[j] += delta
        offspring[i] = np.clip(child, 0.0, 1.0)

    if N % 2 != 0:
        offspring[-1] = population[indices[-1]]
    return offspring

No.3 poor solver’s description and its code:
# Its Description
{Differential Evolution (DE) Crossover: This operator generates an offspring population by applying DE/rand/1 mutation and binomial crossover to each individual in the given population.}
# Its Python Code Implementation of a Function
import numpy as np
def generation(population, F=0.5, CR=0.9):
    N, d = population.shape
    offspring = np.zeros_like(population)

    for i in range(N):
        idxs = np.random.choice([idx for idx in range(N) if idx != i], 3, replace=False)
        x1, x2, x3 = population[idxs]
        trial = population[i].copy()
        j_rand = np.random.randint(d)
        for j in range(d):
            if np.random.rand() < CR or j == j_rand:
                trial[j] = x1[j] + F * (x2[j] - x3[j])
        offspring[i] = np.clip(trial, 0.0, 1.0)
    return offspring

No.4 poor solver’s description and its code:
# Its Description
{Differential Evolution (DE) Crossover: This operator generates an offspring population by applying DE/rand/1 mutation and binomial crossover to each individual in the given population.}
# Its Python Code Implementation of a Function
import numpy as np
def generation(population, F=0.5, CR=0.9):
    N, d = population.shape
    offspring = np.zeros_like(population)

    for i in range(N):
        idxs = np.random.choice([idx for idx in range(N) if idx != i], 3, replace=False)
        x1, x2, x3 = population[idxs]
        trial = population[i].copy()
        j_rand = np.random.randint(d)
        for j in range(d):
            if np.random.rand() < CR or j == j_rand:
                trial[j] = x1[j] + F * (x2[j] - x3[j])
        offspring[i] = np.clip(trial, 0.0, 1.0)
    return offspring

No.5 poor solver’s description and its code:
# Its Description
{Hybrid Crossover Using Adaptive Blend and Self-Adaptive Mutation: This operator combines Gaussian blend crossover with a self-adaptive mutation strategy based on the variance of parents' traits, enhancing exploration and exploitation features in the offspring generation.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population):
    N, d = population.shape
    offspring = np.zeros_like(population)
    
    for i in range(N):
        p1, p2 = population[np.random.choice(N, 2, replace=False)]
        # Gaussian blend crossover
        alpha = np.random.uniform(0, 1, d)
        child = alpha * p1 + (1 - alpha) * p2
        
        # Self-adaptive mutation based on variance
        mutation_strength = np.std([p1, p2], axis=0) * np.random.uniform(0, 1, d)
        mutation_direction = np.random.normal(0, 1, d)  # Gaussian noise
        mutation = mutation_strength * mutation_direction
        child += mutation
        
        # Clip to ensure within [0, 1]
        offspring[i] = np.clip(child, 0.0, 1.0)
    
    return offspring

No.6 poor solver’s description and its code:
# Its Description
{Hybrid Crossover Using Adaptive Blend and Self-Adaptive Mutation: This operator combines Gaussian blend crossover with a self-adaptive mutation strategy based on the variance of parents' traits, enhancing exploration and exploitation features in the offspring generation.}
# Its Python Code Implementation of a Function
import numpy as np

def generation(population):
    N, d = population.shape
    offspring = np.zeros_like(population)
    
    for i in range(N):
        p1, p2 = population[np.random.choice(N, 2, replace=False)]
        # Gaussian blend crossover
        alpha = np.random.uniform(0, 1, d)
        child = alpha * p1 + (1 - alpha) * p2
        
        # Self-adaptive mutation based on variance
        mutation_strength = np.std([p1, p2], axis=0) * np.random.uniform(0, 1, d)
        mutation_direction = np.random.normal(0, 1, d)  # Gaussian noise
        mutation = mutation_strength * mutation_direction
        child += mutation
        
        # Clip to ensure within [0, 1]
        offspring[i] = np.clip(child, 0.0, 1.0)
    
    return offspring


Please create a new generation solver that takes inspiration from the well-performing solvers but avoids the weaknesses and design patterns of the poor-performing solvers.
The new solver should aim for strong performance on optimization tasks.
First, describe the design idea and main steps of your solver in one sentence.
The description must be inside a brace outside the code implementation.
Next, implement it in Python as a function named `generation`.
This function should accept only 1 input: `population`, an array of shape (N, d) of real-valued vectors.
The function should return 1 output: `offspring`, an array of shape (N, d) of real-valued vectors.
The offspring must stay within the bounds [0, 1] for each variable.

Do not give additional explanations.